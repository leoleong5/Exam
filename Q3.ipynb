{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOipojZMpbfJkoh4vhMmpkP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leoleong5/Exam/blob/main/Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3 Discusison/Working"
      ],
      "metadata": {
        "id": "VP0VkyPIrL2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversation with ChatGPT4\n",
        "\n",
        "Human :\n",
        "The task requires you to use Python, along with the Newspaper3k library, which is a simple and efficient tool for extracting and curating articles.\n",
        "\n",
        "Write a Python script that takes a list of URLs as input. Each URL points to a news article.\n",
        "For each URL, your script should extract the article's title and the full text of the article.\n",
        "The output of your script should be a list of dictionaries. Each dictionary should contain the URL, the article title, and the article text.\n",
        "Include error checking in your script to handle possible issues with the URLs or the extraction process.\n",
        "\n",
        "Assistant :\n",
        "\n",
        "Here's a Python script that utilizes the Newspaper3k library to extract news articles' title and text from a given list of URLs. This script also includes error handling for possible issues that might arise during the URL extraction process.\n",
        "\n",
        "Please, make sure to install the Newspaper3k library if you haven't done it yet. You can install it via pip:\n",
        "```\n",
        "pip install newspaper3k\n",
        "```\n",
        "Now here is your Python script:\n",
        "\n",
        "```\n",
        "from newspaper import Article\n",
        "import requests\n",
        "\n",
        "def extract_article_info(urls):\n",
        "    articles = []\n",
        "\n",
        "    for url in urls:\n",
        "        try:\n",
        "            # Check if URL is reachable\n",
        "            request = requests.get(url)\n",
        "            request.raise_for_status()\n",
        "\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "            articles.append({\n",
        "                'url': url,\n",
        "                'title': article.title,\n",
        "                'text': article.text\n",
        "            })\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error encountered while reaching the URL {url}: {str(e)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error encountered while processing the URL {url}: {str(e)}\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "# List of URLs to extract information from\n",
        "urls = ['https://www.example1.com', 'https://www.example2.com', 'https://www.example3.com']\n",
        "\n",
        "articles = extract_article_info(urls)\n",
        "\n",
        "# Print each article's information\n",
        "for article in articles:\n",
        "    print(article)\n",
        "```\n",
        "This script iterates over each URL in the given list, creates an Article object for the URL, downloads the article, and parses it. It then adds a dictionary containing the URL, article title, and article text to the articles list. The extract_article_info function uses try/except blocks to handle exceptions that might occur when trying to reach the URL or during the article extraction process. Please replace the URLs in the urls list with the actual URLs of the news articles you want to extract.\n",
        "\n",
        "Human :\n",
        "Can u explain more about the Newspaper3k library\n",
        "\n",
        "Assistant:\n",
        "\n",
        "Newspaper3k is an advanced news scraping and curation library in Python. It is designed to simplify the process of extracting data from online news articles while still providing a reasonable amount of flexibility and performance. Here's a bit more about what it offers:\n",
        "\n",
        "1. **Article scraping and curation**: Newspaper3k can take in a URL to a news article, download the HTML, and extract useful information like the article's text, authors, images, videos, and more.\n",
        "\n",
        "2. **NLP support**: Once an article is downloaded and parsed, Newspaper3k can perform some natural language processing tasks on the text, such as keyword extraction and summarization. This can be useful for tasks like sentiment analysis or article classification.\n",
        "\n",
        "3. **Multi-language support**: Newspaper3k supports more than 10 languages for article extraction including English, German, French, Chinese, and more. This is useful when working with international news.\n",
        "\n",
        "4. **News outlet extraction**: Apart from extracting articles, Newspaper3k can also extract information from news outlets. For example, given a URL to a news website, it can extract a list of article URLs from that website.\n",
        "\n",
        "Here's a simple example of how to use Newspaper3k:\n",
        "\n",
        "```python\n",
        "from newspaper import Article\n",
        "\n",
        "url = 'http://example.com/some-news-article'\n",
        "article = Article(url)\n",
        "\n",
        "# Download and parse the article\n",
        "article.download()\n",
        "article.parse()\n",
        "\n",
        "# Access the article's properties\n",
        "print(\"Title:\", article.title)\n",
        "print(\"Text:\", article.text)\n",
        "print(\"Authors:\", article.authors)\n",
        "```\n",
        "\n",
        "Note that the Newspaper3k library is not limited to news articles and can be used to extract information from any webpage. However, it's primarily designed for use with news articles, and some features (like author extraction) might not work as well with other types of webpages.\n",
        "\n",
        "Human:\n",
        "Can u explain Explain how your script works and the role of the Newspaper3k library in the script.\n",
        "\n",
        "Assistant:\n",
        "Sure! The provided script leverages the Newspaper3k library to scrape and curate news articles from a list of provided URLs.\n",
        "\n",
        "Here's a detailed breakdown of how the script works and how Newspaper3k fits in:\n",
        "\n",
        "1. **Import the necessary modules**: The script begins by importing the necessary Python modules. The `Article` class from the Newspaper3k library is used to extract and curate articles, and the `requests` module is used to check if each URL is reachable.\n",
        "\n",
        "2. **Define the `extract_article_info` function**: This function takes a list of URLs as an argument and processes each URL to extract article information.\n",
        "\n",
        "3. **URL reachability check**: For each URL, the script first checks whether the URL is reachable by making a GET request using the `requests.get` function. If the URL is not reachable (e.g., the site is down or the URL is incorrect), an exception is raised and caught, and the error message is printed to the console.\n",
        "\n",
        "4. **Download and parse the article**: If the URL is reachable, an `Article` object is created with the URL as an argument. The article is downloaded using the `article.download()` method, and then parsed using the `article.parse()` method.\n",
        "\n",
        "    The `download()` method fetches the HTML content of the article's webpage, and the `parse()` method extracts useful information from the downloaded HTML.\n",
        "\n",
        "5. **Extract article information**: After parsing the article, the script extracts the article's title and full text using the `article.title` and `article.text` attributes, respectively. The URL, title, and text are stored in a dictionary.\n",
        "\n",
        "6. **Handle extraction errors**: If an error occurs during the download, parse, or extraction process (e.g., due to a change in the site's layout or a network issue), an exception is raised and caught, and the error message is printed to the console.\n",
        "\n",
        "7. **Store and return article information**: The dictionary containing the article's information is appended to the `articles` list. After all URLs have been processed, the `extract_article_info` function returns this list.\n",
        "\n",
        "8. **Run the script with a list of URLs**: The script calls the `extract_article_info` function with a list of example URLs and stores the return value in the `articles` variable. It then prints the information for each article to the console.\n",
        "\n",
        "    In this step, you would replace the example URLs with the actual news article URLs you wish to scrape.\n",
        "\n",
        "In summary, the Newspaper3k library is instrumental in this script for downloading, parsing, and extracting information from news articles given their URLs. It greatly simplifies the process of web scraping by handling the complex tasks of fetching HTML content, dealing with various site layouts, and extracting useful information."
      ],
      "metadata": {
        "id": "SDepOujPrQ3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here is my answer"
      ],
      "metadata": {
        "id": "gXMg8JOIsnBC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIFRcA6XpaOQ",
        "outputId": "80bf873a-3647-4313-9109-fe10db041aab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.11.2)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (8.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.2)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.27.1)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-3.4.4-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.12.0)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13541 sha256=faf82691f803e157693dc9a748cb9482062ed8cb950100a471c322d86e5cd4ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=551e03dcca4a2bb7f7b01e2984ee3460c5b4f5ff9470241b4af9854c7249a18e\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398382 sha256=9468a539166a9f7e4aa647713bde09be01b73da84f9fc91b43b4027c572385f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=5143f4459ec4038dfaa95993ba9a13715074cdd062cc49bcafdb4768c9ba9c1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.4.4\n"
          ]
        }
      ],
      "source": [
        "pip install newspaper3k"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Code from ChatGPT4.0\n",
        "from newspaper import Article\n",
        "import requests\n",
        "\n",
        "def extract_article_info(urls):\n",
        "    articles = []\n",
        "\n",
        "    for url in urls:\n",
        "        try:\n",
        "            # Check if URL is reachable\n",
        "            request = requests.get(url)\n",
        "            request.raise_for_status()\n",
        "\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "            articles.append({\n",
        "                'url': url,\n",
        "                'title': article.title,\n",
        "                'text': article.text\n",
        "            })\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error encountered while reaching the URL {url}: {str(e)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error encountered while processing the URL {url}: {str(e)}\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "# List of URLs to extract information from\n",
        "urls = ['https://www.example1.com', 'https://www.example2.com', 'https://www.example3.com']\n",
        "\n",
        "articles = extract_article_info(urls)\n",
        "\n",
        "# Print each article's information\n",
        "for article in articles:\n",
        "    print(article)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmY3Qo9zqnCz",
        "outputId": "c8a74e6e-50ed-4be2-f2c8-76394aca6978"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error encountered while reaching the URL https://www.example1.com: 406 Client Error: Not Acceptable for url: https://www.example1.com/\n",
            "Error encountered while processing the URL https://www.example3.com: Article `download()` failed with 429 Client Error: Too Many Requests for url: https://www.example3.com/ on URL https://www.example3.com\n",
            "{'url': 'https://www.example2.com', 'title': 'My Blog – My WordPress Blog', 'text': 'Welcome to WordPress. This is your first post. Edit or delete it, then start writing!'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Instruction\n",
        "\n",
        "1. Modify the URL list\n",
        "\n",
        "  Locate the following lines in the script:\n",
        "  ```\n",
        "  urls = ['https://www.example1.com', 'https://www.example2.com', 'https://www.example3.com']\n",
        "  ```\n",
        "  Replace the URLs in this list with the URLs of the news articles you want to extract information from.\n",
        "\n",
        "2. Run the script\n",
        "\n",
        "3. Review the output:\n",
        "\n",
        "  The script will print the extracted article information (URL, title, and text). Each article's information is printed as a dictionary. You can copy this information or modify the script.\n"
      ],
      "metadata": {
        "id": "P8ux5GRC0CGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How script works and the role of the Newspaper3k library in the script\n",
        "#### Extracted from ChatGPT4.0\n",
        "\n",
        "1. The script uses the Newspaper3k library to efficiently download, parse, and extract information (i.e., the URL, title, and full text) from online news articles.\n",
        "\n",
        "2. Newspaper3k's `Article` class is used to create an object for each URL. The `download` and `parse` methods of the `Article` class are then used to fetch the HTML content and extract useful information.\n",
        "\n",
        "3. The `requests` module is used to check the reachability of each URL before attempting to download the article. This allows the script to handle unreachable URLs gracefully.\n",
        "\n",
        "4. The script is designed with error handling to deal with potential issues during the reachability check or the extraction process, providing robustness.\n",
        "\n",
        "5. Each article's information is stored as a dictionary in a list, providing an organized and easy-to-use data structure for further analysis or processing.\n",
        "\n",
        "6. The script is flexible and can be easily adapted for a list of URLs of any length, making it a powerful tool for news article data collection."
      ],
      "metadata": {
        "id": "OeqDlfjz6-3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "A5UIVCe_-1Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " ### Code from ChatGPT4.0\n",
        "from newspaper import Article\n",
        "import requests\n",
        "\n",
        "def extract_article_info(urls):\n",
        "    articles = []\n",
        "\n",
        "    for url in urls:\n",
        "        try:\n",
        "            # Check if URL is reachable\n",
        "            request = requests.get(url)\n",
        "            request.raise_for_status()\n",
        "\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "            articles.append({\n",
        "                'url': url,\n",
        "                'title': article.title,\n",
        "                'text': article.text\n",
        "            })\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error encountered while reaching the URL {url}: {str(e)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error encountered while processing the URL {url}: {str(e)}\")\n",
        "\n",
        "    return articles\n",
        "\n",
        "# List of URLs to extract information from\n",
        "urls = ['https://www.bbc.com/news/business-65886509', 'https://www.bbc.com/news/world-asia-india-65880173', 'https://www.example3.com']\n",
        "\n",
        "articles = extract_article_info(urls)\n",
        "\n",
        "# Print each article's information\n",
        "for article in articles:\n",
        "    print(article)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bkwDWLQ-391",
        "outputId": "05d0f6e3-b8ba-4eb6-9bc9-8ec8f4d49a60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error encountered while processing the URL https://www.example3.com: Article `download()` failed with 429 Client Error: Too Many Requests for url: https://www.example3.com/ on URL https://www.example3.com\n",
            "{'url': 'https://www.bbc.com/news/business-65886509', 'title': \"Yaccarino: Twitter to be 'most accurate real-time info source'\", 'text': 'Allow Twitter content?\\n\\nThis article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitter’s cookie policy, external and privacy policy, external before accepting. To view this content choose ‘accept and continue’.'}\n",
            "{'url': 'https://www.bbc.com/news/world-asia-india-65880173', 'title': \"Covid database: India's health ministry denies major breach\", 'text': 'Allow Twitter content?\\n\\nThis article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitter’s cookie policy, external and privacy policy, external before accepting. To view this content choose ‘accept and continue’.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the test cases conducted, it is evident that when provided with valid URLs, the script accurately extracts and outputs the URLs, titles, and text of the corresponding news articles. However, when an invalid URL is provided, the script appropriately catches the error and outputs a corresponding error message."
      ],
      "metadata": {
        "id": "6Jlg-O4x_H2M"
      }
    }
  ]
}